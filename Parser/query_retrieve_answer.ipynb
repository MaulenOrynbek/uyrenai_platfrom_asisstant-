{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a3f015f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чтобы стать инженером машинного обучения, я рекомендую вам начать с курса по аналитике данных. Этот курс поможет вам освоить базовые навыки анализа данных, которые являются основой для дальнейшего изучения машинного обучения. Вы узнаете о работе аналитика данных, получите практические навыки и познакомитесь с основными инструментами. \n",
      "\n",
      "После завершения курса по аналитике данных, вы можете перейти к более специализированным курсам по машинному обучению, если они доступны на платформе.\n"
     ]
    }
   ],
   "source": [
    "# Load FAISS artifacts, perform vector search for a user query, and answer with OpenAI Chat\n",
    "# !pip install -q numpy faiss-cpu openai==1.*\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from openai import OpenAI\n",
    "\n",
    "# Config\n",
    "CACHE_DIR = \"rag_cache\"\n",
    "INDEX_PATH = os.path.join(CACHE_DIR, \"faiss.index\")\n",
    "IDMAP_PATH = os.path.join(CACHE_DIR, \"id_map.npy\")\n",
    "TEXTS_PATH = os.path.join(CACHE_DIR, \"rag_texts.jsonl\")\n",
    "MANIFEST_PATH = os.path.join(CACHE_DIR, \"manifest.json\")\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "LLM_MODEL = \"gpt-4o-mini\"\n",
    "TOP_K = 5\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise RuntimeError(\"Set OPENAI_API_KEY in your environment before running this cell.\")\n",
    "\n",
    "# 1) Load artifacts\n",
    "if not (os.path.exists(INDEX_PATH) and os.path.exists(IDMAP_PATH) and os.path.exists(TEXTS_PATH)):\n",
    "    raise RuntimeError(\"Artifacts not found. Run the embedding/build step first (see Untitled-2.ipynb).\")\n",
    "\n",
    "index = faiss.read_index(INDEX_PATH)\n",
    "id_map = np.load(IDMAP_PATH)\n",
    "texts = {}\n",
    "with open(TEXTS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        texts[int(obj[\"id\"])] = obj[\"rag_text\"]\n",
    "\n",
    "# 2) Helper functions\n",
    "client = OpenAI()\n",
    "\n",
    "def embed_query(query: str) -> np.ndarray:\n",
    "    resp = client.embeddings.create(model=EMBED_MODEL, input=[query])\n",
    "    q = np.array(resp.data[0].embedding, dtype=np.float32)\n",
    "    q = q / np.linalg.norm(q)\n",
    "    return q\n",
    "\n",
    "def search(q_vec: np.ndarray, k: int = TOP_K):\n",
    "    scores, idxs = index.search(q_vec.reshape(1, -1), k)\n",
    "    return scores[0], id_map[idxs[0]]\n",
    "\n",
    "def format_context(hit_ids, max_chars_per_chunk: int = 2000) -> str:\n",
    "    blocks = []\n",
    "    for hid in hit_ids:\n",
    "        txt = texts.get(int(hid), \"\")\n",
    "        if max_chars_per_chunk and len(txt) > max_chars_per_chunk:\n",
    "            txt = txt[:max_chars_per_chunk] + \"\\n...\"\n",
    "        blocks.append(f\"[id={hid}]\\n{txt}\")\n",
    "    return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "\n",
    "def answer_with_rag(query: str, k: int = TOP_K, temperature: float = 0.2) -> str:\n",
    "    q = embed_query(query)\n",
    "    scores, hit_ids = search(q, k)\n",
    "    context = format_context(hit_ids.tolist())\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant. That helps to navigate throughout the platform called uyren.ai. The is an online learning platform. With a bunch of courses \"\n",
    "        \"You answer the requests based on the data you get.\"\n",
    "    )\n",
    "    user_prompt = f\"Question:\\n{query}\\n\\nContext:\\n{context}\\n\\nProvide a concise answer.\"\n",
    "\n",
    "    chat = client.chat.completions.create(\n",
    "        model=LLM_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return chat.choices[0].message.content\n",
    "\n",
    "# 3) Example usage\n",
    "user_query = \"Я хочу стать инжереном машиннего обучения. Какие курсы мне стоит взять из вашей платформы, чтобы начать свой путь?\"\n",
    "print(answer_with_rag(user_query, k=TOP_K))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
